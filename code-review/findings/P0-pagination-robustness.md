# P0: åˆ†é¡µå¤±è´¥ä¸é”™è¯¯æ¢å¤æœºåˆ¶ç¼ºé™·

**ä¼šè¯**: Session 2.2 - åˆ†é¡µä¸é”™è¯¯æ¢å¤å®¡æŸ¥
**å®¡æŸ¥æ—¥æœŸ**: 2026-01-11
**ä¸¥é‡ç¨‹åº¦**: P0 (Critical)
**ç±»åˆ«**: Reliability / Data Integrity / AWS Integration

---

## æ‰§è¡Œæ‘˜è¦ (Executive Summary)

RepliMap çš„æ‰«æå™¨å­˜åœ¨ **åˆ†é¡µå¤±è´¥å¯¼è‡´æ•´ä½“æ‰«æä¸­æ­¢** çš„ä¸¥é‡é—®é¢˜ï¼Œè¿™åœ¨å¤§è§„æ¨¡ AWS ç¯å¢ƒä¸­ä¼šå¯¼è‡´æ•°æ®ä¸å®Œæ•´ã€æ‰«æä¸å¯é ï¼Œä¸¥é‡å½±å“äº§å“å¯ç”¨æ€§ã€‚

**æ ¸å¿ƒé—®é¢˜**: æ‰€æœ‰æ‰«æå™¨ä½¿ç”¨ AWS paginatorï¼Œä½† **å•é¡µå¤±è´¥ = æ•´ä¸ªèµ„æºç±»å‹æ‰«æå¤±è´¥**ï¼Œæ²¡æœ‰éƒ¨åˆ†æˆåŠŸä¿å­˜æœºåˆ¶ã€‚

**å½±å“**:
- ğŸ”´ **æ•°æ®å®Œæ•´æ€§é£é™©**: å¤§å‹è´¦æˆ·æ‰«æå¯èƒ½æ¼æ‰æ•°ç™¾ä¸ªèµ„æº
- ğŸ”´ **å•†ä¸šå½±å“**: å®¢æˆ·å¯¹å·¥å…·å¯é æ€§å¤±å»ä¿¡å¿ƒï¼Œå½±å“ç»­è´¹
- ğŸ”´ **ç”¨æˆ·ä½“éªŒ**: æ‰«æä¸ç¡®å®šæ€§é«˜ï¼Œéœ€è¦å¤šæ¬¡é‡è¯•

**å‘ç°æ•°é‡**: 6 ä¸ª P0/P1 é—®é¢˜
**ä¿®å¤ä¼˜å…ˆçº§**: ç«‹å³ä¿®å¤ (P0)ï¼Œä¸é€Ÿç‡é™åˆ¶é—®é¢˜åŒç­‰ä¼˜å…ˆçº§

---

## åˆ†é¡µä½¿ç”¨æƒ…å†µçŸ©é˜µ (Pagination Usage Matrix)

| æ‰«æå™¨ | Paginator API è°ƒç”¨ | é¡µæ•°ä¼°ç®— (1000èµ„æº) | é”™è¯¯å¤„ç† | éƒ¨åˆ†ä¿å­˜ | é£é™©ç­‰çº§ |
|--------|-------------------|---------------------|----------|----------|----------|
| **VPCScanner** | describe_vpcs<br>describe_subnets<br>describe_security_groups<br>describe_flow_logs | 1-2 é¡µ<br>5-10 é¡µ<br>10-20 é¡µ<br>10+ é¡µ | âŒ æ‰«æå™¨çº§åˆ« try/except | âŒ æ—  | ğŸ”´ High |
| **EC2Scanner** | describe_instances | 20-50 é¡µ (æ¯é¡µ20å®ä¾‹) | âŒ æ‰«æå™¨çº§åˆ« | âŒ æ—  | ğŸ”´ Critical |
| **RDSScanner** | describe_db_instances<br>describe_db_subnet_groups | 5-10 é¡µ<br>2-5 é¡µ | âŒ æ‰«æå™¨çº§åˆ« | âŒ æ—  | ğŸ”´ High |
| **ComputeScanner** | describe_launch_templates<br>describe_target_groups<br>describe_load_balancers<br>describe_auto_scaling_groups | 5-10 é¡µ<br>10-20 é¡µ<br>5-10 é¡µ<br>5-10 é¡µ | âŒ æ‰«æå™¨çº§åˆ« | âŒ æ—  | ğŸ”´ Critical |
| **StorageScanner** | describe_volumes | 20-50 é¡µ | âŒ æ‰«æå™¨çº§åˆ« | âŒ æ—  | ğŸ”´ High |
| **NetworkingScanner** | describe_internet_gateways<br>describe_nat_gateways<br>describe_route_tables<br>describe_vpc_endpoints<br>describe_network_acls | 1-2 é¡µ<br>2-5 é¡µ<br>10-20 é¡µ<br>5-10 é¡µ<br>5-10 é¡µ | âŒ æ‰«æå™¨çº§åˆ« | âŒ æ—  | ğŸŸ¡ Medium |
| **IAMScanner** | list_roles<br>list_instance_profiles | 5-10 é¡µ<br>2-5 é¡µ | âŒ æ‰«æå™¨çº§åˆ« | âŒ æ—  | ğŸŸ¡ Medium |
| **S3Scanner** | list_buckets (éåˆ†é¡µ!) | N/A (ç¡¬é™åˆ¶1000æ¡¶) | âŒ å•æ¬¡è°ƒç”¨ | âŒ æ—  | ğŸ”´ Critical |
| **MessagingScanner** | list_queues<br>list_topics | 5-10 é¡µ<br>5-10 é¡µ | âŒ æ‰«æå™¨çº§åˆ« | âŒ æ—  | ğŸŸ¡ Medium |
| **ElastiCacheScanner** | describe_cache_clusters<br>describe_cache_subnet_groups | 5-10 é¡µ<br>2-5 é¡µ | âŒ æ‰«æå™¨çº§åˆ« | âŒ æ—  | ğŸŸ¡ Medium |
| **MonitoringScanner** | describe_log_groups<br>describe_alarms | 10-20 é¡µ<br>5-10 é¡µ | âŒ æ‰«æå™¨çº§åˆ« | âŒ æ—  | ğŸŸ¢ Low |

**ç»Ÿè®¡**:
- **ä½¿ç”¨åˆ†é¡µå™¨çš„æ‰«æå™¨**: 12 ä¸ª
- **æ­£ç¡®çš„é€é¡µé”™è¯¯å¤„ç†**: 0 ä¸ª (0%)
- **æ”¯æŒéƒ¨åˆ†æˆåŠŸä¿å­˜**: 0 ä¸ª (0%)
- **S3 list_buckets ç¡¬é™åˆ¶**: 1000 æ¡¶ (æ— åˆ†é¡µï¼)

---

## [FINDING-PG001] åˆ†é¡µå™¨é”™è¯¯å¤„ç†åœ¨æ‰«æå™¨çº§åˆ«ï¼Œå•é¡µå¤±è´¥å¯¼è‡´æ•´ä½“ä¸­æ­¢ ğŸ”¥

**ä¸¥é‡ç¨‹åº¦**: Critical
**ä¼˜å…ˆçº§**: P0
**ç±»åˆ«**: Reliability / Data Integrity
**ç»„ä»¶**: [replimap/scanners/vpc_scanner.py](../replimap/scanners/vpc_scanner.py):62-90, [replimap/scanners/ec2_scanner.py](../replimap/scanners/ec2_scanner.py):62-70

### æè¿°

æ‰€æœ‰æ‰«æå™¨çš„åˆ†é¡µé€»è¾‘æ¨¡å¼ä¸ºï¼š

```python
# replimap/scanners/vpc_scanner.py:62-90
def _scan_vpcs(self, ec2: Any, graph: GraphEngine) -> None:
    """Scan all VPCs in the region."""
    logger.debug("Scanning VPCs...")

    # ğŸ”´ é—®é¢˜1: Flow logs åˆ†é¡µå¤±è´¥ â†’ æ•´ä¸ª VPC æ‰«æä¸­æ­¢
    try:
        fl_paginator = ec2.get_paginator("describe_flow_logs")
        for fl_page in fl_paginator.paginate():
            for flow_log in fl_page.get("FlowLogs", []):
                # å¤„ç† flow log
    except ClientError as e:
        logger.debug(f"Could not describe flow logs: {e}")
        # âš ï¸ åªæ˜¯ logï¼Œä½†å¦‚æœè¿™é‡ŒæŠ›å‡ºå¼‚å¸¸ä¼šæ€æ ·ï¼Ÿ

    # ğŸ”´ é—®é¢˜2: VPC åˆ†é¡µæœ¬èº«æ²¡æœ‰é”™è¯¯å¤„ç†
    paginator = ec2.get_paginator("describe_vpcs")
    for page in paginator.paginate():  # âŒ å¦‚æœç¬¬5é¡µå¤±è´¥ï¼Œå‰4é¡µçš„æ•°æ®ä¸¢å¤±
        for vpc in page.get("Vpcs", []):
            # ... å¤„ç† VPC
            graph.add_resource(node)  # æ•°æ®å·²æ·»åŠ åˆ°å›¾ä¸­
```

**é—®é¢˜åˆ†æ**:

1. **åˆ†é¡µå¾ªç¯æ— é”™è¯¯åŒ…è£…**:
   ```python
   for page in paginator.paginate():  # âŒ æ²¡æœ‰ try/except
       for vpc in page.get("Vpcs", []):
           graph.add_resource(node)
   ```
   - å¦‚æœç¬¬ N é¡µ (N > 1) å› ç½‘ç»œè¶…æ—¶/AWS é™æµå¤±è´¥
   - å‰ N-1 é¡µçš„æ•°æ® **å·²ç»æ·»åŠ åˆ° graph**
   - ä½†æ‰«æå™¨å¤–å±‚çš„ `try/except ClientError` ä¼šæ•è·é”™è¯¯ â†’ **æ•´ä¸ªæ‰«ææ ‡è®°ä¸ºå¤±è´¥**
   - **ç»“æœ**: éƒ¨åˆ†æ•°æ®åœ¨å›¾ä¸­ï¼Œä½†æ‰«ææŠ¥å‘Šæ˜¾ç¤ºå¤±è´¥ï¼Œç”¨æˆ·ä¸çŸ¥é“æ•°æ®æ˜¯å¦å®Œæ•´

2. **åµŒå¥—åˆ†é¡µæ— ç‹¬ç«‹é”™è¯¯å¤„ç†** (VPC flow logs ç¤ºä¾‹):
   ```python
   try:
       fl_paginator = ec2.get_paginator("describe_flow_logs")
       for fl_page in fl_paginator.paginate():
           # ...
   except ClientError as e:
       logger.debug(f"Could not describe flow logs: {e}")
   ```
   - Flow logs å¤±è´¥åªæ˜¯ debug æ—¥å¿—
   - ä½†å¦‚æœ paginate() æœ¬èº«æŠ›å‡ºå¼‚å¸¸ï¼ˆä¸æ˜¯ ClientErrorï¼‰ï¼Œä¼šå‘ä¸Šä¼ æ’­
   - å¯¼è‡´ VPC æ‰«æä¸­æ­¢

### å½±å“

**æ•°æ®å®Œæ•´æ€§é£é™©**:
- å¤§å‹è´¦æˆ· (500+ EC2 å®ä¾‹ â†’ 25+ é¡µ):
  - ç¬¬ 20 é¡µå› ä¸´æ—¶ç½‘ç»œé—®é¢˜å¤±è´¥
  - å‰ 19 é¡µ (380 å®ä¾‹) **å·²æ·»åŠ åˆ°å›¾ä¸­**
  - ç”¨æˆ·çœ‹åˆ° "EC2Scanner failed"
  - **å®é™…æƒ…å†µ**: æœ‰ 380/500 å®ä¾‹ï¼Œä½†ç”¨æˆ·ä»¥ä¸ºä¸€ä¸ªéƒ½æ²¡æœ‰

**å•†ä¸šå½±å“**:
- å®¢æˆ·è®¤ä¸ºå·¥å…·ä¸å¯é  â†’ ä¸æ„¿ç»­è´¹
- Support ticket å¢åŠ  ("ä¸ºä»€ä¹ˆæˆ‘çš„ EC2 æ‰«æå¤±è´¥ï¼Ÿ")
- ç«å“å¯¹æ¯”æ—¶å¤„äºåŠ£åŠ¿

**ç”¨æˆ·å½±å“**:
- éœ€è¦å¤šæ¬¡é‡è¯•æ‰«æ (æµªè´¹æ—¶é—´å’Œ AWS API é…é¢)
- ä¸ç¡®å®šæ•°æ®æ˜¯å¦å®Œæ•´
- ç”Ÿæˆçš„ Terraform ä»£ç å¯èƒ½ç¼ºå°‘èµ„æº

### è¯æ®

**ä»£ç è·¯å¾„ 1: VPCScanner**
```
VPCScanner.scan()
  â””â”€> _scan_vpcs()
      â”œâ”€> describe_flow_logs paginator (try/except)
      â””â”€> describe_vpcs paginator (âŒ æ— é”™è¯¯å¤„ç†)
          â””â”€> æ¯é¡µ: graph.add_resource(vpc_node)
          â””â”€> å¦‚æœç¬¬ N é¡µå¤±è´¥:
              - å‰ N-1 é¡µæ•°æ®åœ¨å›¾ä¸­ âœ…
              - å¼‚å¸¸å‘ä¸Šä¼ æ’­åˆ° scan() çš„ except ClientError
              - æ‰«ææ ‡è®°ä¸ºå¤±è´¥ âŒ
```

**ä»£ç è·¯å¾„ 2: EC2Scanner**
```python
# replimap/scanners/ec2_scanner.py:62-70
def _scan_instances(self, ec2: Any, graph: GraphEngine) -> None:
    """Scan all EC2 instances in the region."""
    logger.debug("Scanning EC2 instances...")

    paginator = ec2.get_paginator("describe_instances")
    for page in paginator.paginate():  # âŒ æ— é”™è¯¯å¤„ç†
        for reservation in page.get("Reservations", []):
            for instance in reservation.get("Instances", []):
                self._process_instance(instance, graph)
```

**å®é™…åœºæ™¯æ¨¡æ‹Ÿ**:

| è´¦æˆ·è§„æ¨¡ | é¡µæ•° | å¤±è´¥é¡µ | å·²ä¿å­˜èµ„æº | æŠ¥å‘ŠçŠ¶æ€ | ç”¨æˆ·æ„ŸçŸ¥ |
|---------|------|--------|-----------|---------|---------|
| 100 EC2 | 5 é¡µ | ç¬¬ 3 é¡µ | 40 ä¸ª | âŒ Failed | ä»¥ä¸º 0 ä¸ª |
| 500 EC2 | 25 é¡µ | ç¬¬ 20 é¡µ | 380 ä¸ª | âŒ Failed | ä»¥ä¸º 0 ä¸ª |
| 1000 RDS | 50 é¡µ | ç¬¬ 45 é¡µ | 880 ä¸ª | âŒ Failed | ä»¥ä¸º 0 ä¸ª |

### å¤ç°æ­¥éª¤

1. å‡†å¤‡ä¸€ä¸ªå¤§å‹ AWS è´¦æˆ· (500+ EC2 å®ä¾‹)
2. æ¨¡æ‹Ÿç½‘ç»œä¸ç¨³å®šç¯å¢ƒ:
   ```bash
   # ä½¿ç”¨ tc (traffic control) æ¨¡æ‹Ÿ 5% ä¸¢åŒ…ç‡
   sudo tc qdisc add dev eth0 root netem loss 5%
   ```
3. è¿è¡Œæ‰«æ:
   ```bash
   replimap -p large-account -r us-east-1 scan
   ```
4. è§‚å¯Ÿæ—¥å¿—:
   ```
   INFO: Scanning EC2 instances in us-east-1...
   DEBUG: Processing page 1 (20 instances)
   DEBUG: Processing page 2 (20 instances)
   ...
   DEBUG: Processing page 18 (20 instances)
   ERROR: EC2 scanning failed: Read timeout on endpoint URL
   ```
5. æ£€æŸ¥å›¾ä¸­çš„å®ä¾‹æ•°é‡:
   ```python
   # å®é™…æœ‰ 360 ä¸ªå®ä¾‹åœ¨å›¾ä¸­ï¼Œä½†ç”¨æˆ·ä¸çŸ¥é“
   ```

### æ¨èä¿®å¤

**æ–¹æ¡ˆ 1: é€é¡µé”™è¯¯å¤„ç† + éƒ¨åˆ†æˆåŠŸæŠ¥å‘Š** â­ æ¨è

```python
# replimap/scanners/base.py (æ–°å¢å·¥å…·å‡½æ•°)
from dataclasses import dataclass
from typing import Iterator, TypeVar, Callable

T = TypeVar('T')

@dataclass
class PaginationResult:
    """åˆ†é¡µç»“æœï¼ŒåŒ…å«æˆåŠŸå’Œå¤±è´¥ç»Ÿè®¡"""
    total_pages: int = 0
    successful_pages: int = 0
    failed_pages: int = 0
    items_collected: int = 0
    errors: list[Exception] = None

    def __post_init__(self):
        if self.errors is None:
            self.errors = []

    @property
    def success_rate(self) -> float:
        if self.total_pages == 0:
            return 0.0
        return self.successful_pages / self.total_pages

def resilient_paginate(
    paginator,
    result_key: str,
    on_page_success: Callable[[list], None] | None = None,
    max_retries: int = 3,
) -> PaginationResult:
    """
    åˆ†é¡µå™¨åŒ…è£…å™¨ï¼Œæ”¯æŒé€é¡µé‡è¯•å’Œéƒ¨åˆ†æˆåŠŸã€‚

    Args:
        paginator: boto3 paginator å¯¹è±¡
        result_key: é¡µé¢ç»“æœçš„ key (å¦‚ "Vpcs", "Instances")
        on_page_success: æ¯é¡µæˆåŠŸåçš„å›è°ƒå‡½æ•°
        max_retries: æ¯é¡µæœ€å¤§é‡è¯•æ¬¡æ•°

    Returns:
        PaginationResult åŒ…å«ç»Ÿè®¡ä¿¡æ¯

    Example:
        paginator = ec2.get_paginator("describe_vpcs")
        result = resilient_paginate(
            paginator.paginate(),
            "Vpcs",
            on_page_success=lambda vpcs: [graph.add_resource(v) for v in vpcs]
        )

        if result.success_rate < 0.8:
            logger.warning(f"Only {result.success_rate:.0%} pages succeeded")
    """
    result = PaginationResult()
    page_iterator = iter(paginator)

    while True:
        page_num = result.total_pages + 1
        last_error = None

        # é€é¡µé‡è¯•
        for attempt in range(max_retries + 1):
            try:
                page = next(page_iterator)
                result.total_pages += 1

                # æå–ç»“æœ
                items = page.get(result_key, [])
                result.items_collected += len(items)

                # è°ƒç”¨æˆåŠŸå›è°ƒ
                if on_page_success and items:
                    on_page_success(items)

                result.successful_pages += 1
                logger.debug(
                    f"Page {page_num}: {len(items)} items "
                    f"(total: {result.items_collected})"
                )
                break  # æˆåŠŸï¼Œç»§ç»­ä¸‹ä¸€é¡µ

            except StopIteration:
                # åˆ†é¡µç»“æŸ
                return result

            except Exception as e:
                last_error = e

                if attempt < max_retries:
                    backoff = (2 ** attempt) + random.uniform(0, 1)
                    logger.warning(
                        f"Page {page_num} failed (attempt {attempt + 1}/{max_retries + 1}), "
                        f"retrying in {backoff:.1f}s: {e}"
                    )
                    time.sleep(backoff)
                else:
                    # æœ€å¤§é‡è¯•æ¬¡æ•°ç”¨å°½
                    result.failed_pages += 1
                    result.errors.append(last_error)
                    logger.error(
                        f"Page {page_num} failed after {max_retries + 1} attempts: {e}"
                    )
                    # âš ï¸ ç»§ç»­ä¸‹ä¸€é¡µï¼Œä¸ä¸­æ­¢æ•´ä¸ªæ‰«æ
                    try:
                        page_iterator = iter([next(page_iterator)])  # å°è¯•ç»§ç»­
                    except StopIteration:
                        return result

    return result
```

**ä¿®æ”¹ VPCScanner**:

```python
# replimap/scanners/vpc_scanner.py (ä¿®æ”¹)
def _scan_vpcs(self, ec2: Any, graph: GraphEngine) -> None:
    """Scan all VPCs in the region."""
    logger.debug("Scanning VPCs...")

    # Flow logs (å¯é€‰ï¼Œå¤±è´¥ä¸å½±å“ VPC æ‰«æ)
    vpc_flow_logs: dict[str, list[dict[str, Any]]] = {}
    fl_paginator = ec2.get_paginator("describe_flow_logs")
    fl_result = resilient_paginate(
        fl_paginator.paginate(),
        "FlowLogs",
        on_page_success=lambda logs: self._process_flow_logs(logs, vpc_flow_logs)
    )

    if fl_result.failed_pages > 0:
        logger.warning(
            f"Flow logs: {fl_result.successful_pages}/{fl_result.total_pages} pages succeeded"
        )

    # VPCs (ä¸»è¦èµ„æº)
    vpc_paginator = ec2.get_paginator("describe_vpcs")
    vpc_result = resilient_paginate(
        vpc_paginator.paginate(),
        "Vpcs",
        on_page_success=lambda vpcs: self._process_vpcs(vpcs, graph, vpc_flow_logs)
    )

    # ğŸ”¥ å…³é”®ï¼šæ ¹æ®æˆåŠŸç‡å†³å®šæ˜¯å¦æŠ¥å‘Šä¸ºå¤±è´¥
    if vpc_result.success_rate < 0.5:  # å°‘äº50%é¡µé¢æˆåŠŸ â†’ ä¸¥é‡å¤±è´¥
        raise RuntimeError(
            f"VPC scan critically failed: only {vpc_result.successful_pages}/{vpc_result.total_pages} "
            f"pages succeeded ({vpc_result.items_collected} VPCs collected)"
        )
    elif vpc_result.failed_pages > 0:
        logger.warning(
            f"âš ï¸ VPC scan partially succeeded: {vpc_result.successful_pages}/{vpc_result.total_pages} "
            f"pages ({vpc_result.items_collected} VPCs, {vpc_result.failed_pages} pages failed)"
        )

def _process_flow_logs(self, logs: list, vpc_flow_logs: dict):
    """å¤„ç† flow log é¡µé¢"""
    for flow_log in logs:
        resource_id = flow_log.get("ResourceId", "")
        if resource_id.startswith("vpc-"):
            if resource_id not in vpc_flow_logs:
                vpc_flow_logs[resource_id] = []
            vpc_flow_logs[resource_id].append({
                "flow_log_id": flow_log.get("FlowLogId"),
                "traffic_type": flow_log.get("TrafficType"),
                # ...
            })

def _process_vpcs(self, vpcs: list, graph: GraphEngine, vpc_flow_logs: dict):
    """å¤„ç† VPC é¡µé¢"""
    for vpc in vpcs:
        vpc_id = vpc["VpcId"]
        tags = self._extract_tags(vpc.get("Tags"))
        flow_logs = vpc_flow_logs.get(vpc_id, [])

        node = ResourceNode(
            id=vpc_id,
            resource_type=ResourceType.VPC,
            region=self.region,
            config={
                "cidr_block": vpc["CidrBlock"],
                "flow_logs_enabled": len(flow_logs) > 0,
                "flow_logs": flow_logs,
                # ...
            },
            arn=f"arn:aws:ec2:{self.region}:{self._get_account_id(vpc)}:vpc/{vpc_id}",
            tags=tags,
        )

        graph.add_resource(node)
        logger.debug(f"Added VPC: {vpc_id}")
```

**æ–¹æ¡ˆ 2: åˆ†é¡µçŠ¶æ€æŒä¹…åŒ– (é•¿æœŸä¼˜åŒ–)**

å¯¹äºè¶…å¤§è´¦æˆ·ï¼Œæ”¯æŒæ‰«æä¸­æ–­åä»ä¸Šæ¬¡ä½ç½®æ¢å¤ï¼š

```python
@dataclass
class ScanCheckpoint:
    """æ‰«ææ£€æŸ¥ç‚¹"""
    scanner_name: str
    resource_type: str
    last_page_token: str | None
    items_scanned: int
    timestamp: float

class CheckpointManager:
    """ç®¡ç†æ‰«ææ£€æŸ¥ç‚¹"""

    def save_checkpoint(self, checkpoint: ScanCheckpoint):
        """ä¿å­˜åˆ° SQLite/JSON"""
        pass

    def load_checkpoint(self, scanner_name: str, resource_type: str) -> ScanCheckpoint | None:
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        pass

    def clear_checkpoint(self, scanner_name: str, resource_type: str):
        """æ¸…é™¤æ£€æŸ¥ç‚¹"""
        pass

# ä½¿ç”¨ç¤ºä¾‹
def _scan_vpcs_resumable(self, ec2, graph):
    checkpoint_mgr = CheckpointManager()
    checkpoint = checkpoint_mgr.load_checkpoint("VPCScanner", "aws_vpc")

    paginator = ec2.get_paginator("describe_vpcs")
    pagination_config = {}
    if checkpoint and checkpoint.last_page_token:
        pagination_config['StartingToken'] = checkpoint.last_page_token
        logger.info(f"Resuming VPC scan from checkpoint ({checkpoint.items_scanned} already scanned)")

    page_iterator = paginator.paginate(**pagination_config)

    for page in page_iterator:
        # å¤„ç†é¡µé¢
        # ...

        # ä¿å­˜æ£€æŸ¥ç‚¹
        checkpoint_mgr.save_checkpoint(ScanCheckpoint(
            scanner_name="VPCScanner",
            resource_type="aws_vpc",
            last_page_token=page.get('NextToken'),
            items_scanned=items_scanned,
            timestamp=time.time()
        ))
```

### å·¥ä½œé‡ä¼°ç®—

**æ–¹æ¡ˆ 1 (resilient_paginate + éƒ¨åˆ†æˆåŠŸæŠ¥å‘Š)**:
- **å¼€å‘æ—¶é—´**: 2-3 å¤©
  - å®ç° `resilient_paginate()`: 6 å°æ—¶
  - ä¿®æ”¹ VPCScanner (ç¤ºä¾‹): 2 å°æ—¶
  - ä¿®æ”¹å…¶ä»– 11 ä¸ªæ‰«æå™¨: 12 å°æ—¶ (æ¯ä¸ª 1 å°æ—¶)
  - å•å…ƒæµ‹è¯• (æ¨¡æ‹Ÿåˆ†é¡µå¤±è´¥): 4 å°æ—¶
- **æµ‹è¯•æ—¶é—´**: 1 å¤©
  - é›†æˆæµ‹è¯• (çœŸå®è´¦æˆ·): 4 å°æ—¶
  - Chaos testing (æ¨¡æ‹Ÿç½‘ç»œæ•…éšœ): 4 å°æ—¶
- **æ€»è®¡**: **3-4 å¤©** (çº¦ 7 ä¸ª story points)

**æ–¹æ¡ˆ 2 (æ£€æŸ¥ç‚¹æ¢å¤)**:
- **å¼€å‘æ—¶é—´**: 5-7 å¤©
- **é£é™©**: ä¸­ç­‰ï¼ˆçŠ¶æ€æŒä¹…åŒ–å¤æ‚æ€§ï¼‰
- **å»ºè®®**: P1 ä¼˜å…ˆçº§ï¼Œé•¿æœŸä¼˜åŒ–

### ä¾èµ–

- éœ€è¦ä¸ [FINDING-RL001] (é€Ÿç‡é™åˆ¶) åè°ƒï¼šå¦‚æœåˆ†é¡µå¤±è´¥æ˜¯å› ä¸º throttlingï¼Œåº”è¯¥å…ˆä¿® RL001
- ä¸ [FINDING-RL002] (é‡è¯•é€»è¾‘) æœ‰äº¤äº’ï¼šresilient_paginate çš„é‡è¯•ä¸å…¨å±€é‡è¯•è£…é¥°å™¨å†²çª

---

## [FINDING-PG002] S3 list_buckets ç¡¬é™åˆ¶ 1000 æ¡¶ï¼Œæ— åˆ†é¡µæ”¯æŒ ğŸ”¥

**ä¸¥é‡ç¨‹åº¦**: Critical
**ä¼˜å…ˆçº§**: P0
**ç±»åˆ«**: Data Completeness
**ç»„ä»¶**: [replimap/scanners/s3_scanner.py](../replimap/scanners/s3_scanner.py):57-65

### æè¿°

S3 `list_buckets` API **ä¸æ”¯æŒåˆ†é¡µ**ï¼Œå•æ¬¡è°ƒç”¨æœ€å¤šè¿”å› 1000 ä¸ªæ¡¶ã€‚

```python
# replimap/scanners/s3_scanner.py:57-65
def _scan_buckets(self, s3: Any, graph: GraphEngine) -> None:
    """Scan all S3 buckets with parallel processing."""
    logger.debug("Listing S3 buckets...")

    try:
        response = s3.list_buckets()  # ğŸ”´ æœ€å¤šè¿”å› 1000 ä¸ªæ¡¶ï¼
    except ClientError as e:
        self._handle_aws_error(e, "list S3 buckets")
        return

    # ğŸ”´ å¦‚æœè´¦æˆ·æœ‰ 1001+ æ¡¶ï¼Œè¿™é‡Œåªèƒ½çœ‹åˆ° 1000 ä¸ª
    for bucket in response.get("Buckets", []):
        bucket_name = bucket["Name"]
        # ...
```

**AWS å®˜æ–¹æ–‡æ¡£**:
> "list_buckets returns up to 1000 buckets. For accounts with more buckets, you must use the S3 Control API or AWS Organizations."

### å½±å“

**æ•°æ®ä¸¢å¤±**:
- å¤§å‹ç»„ç»‡ (å¤šå›¢é˜Ÿã€å¤šäº§å“) å¯èƒ½æœ‰ 1000+ S3 æ¡¶
- RepliMap é™é»˜è·³è¿‡ç¬¬ 1001+ ä¸ªæ¡¶
- ç”Ÿæˆçš„ Terraform ä»£ç ç¼ºå°‘è¿™äº›æ¡¶

**å•†ä¸šå½±å“**:
- ä¼ä¸šå®¢æˆ·ä¸é€‚ç”¨ï¼ˆS3 æ¡¶æ•°é‡é€šå¸¸å¾ˆå¤šï¼‰
- ç«å“ (å¦‚ Terraformer) å¯èƒ½æ²¡æœ‰è¿™ä¸ªé™åˆ¶

### è¯æ®

**AWS æ–‡æ¡£è¯æ®**:
- [list_buckets API Reference](https://docs.aws.amazon.com/AmazonS3/latest/API/API_ListBuckets.html)
- "Returns a list of all buckets owned by the authenticated sender of the request. **To list more than 1000 buckets, you must use the S3 Control API.**"

**å®é™…æµ‹è¯•**:
```python
import boto3

s3 = boto3.client('s3')
response = s3.list_buckets()

print(f"Buckets returned: {len(response['Buckets'])}")
# è¾“å‡º: Buckets returned: 1000 (å³ä½¿è´¦æˆ·æœ‰ 1500 ä¸ªæ¡¶)
```

### æ¨èä¿®å¤

**æ–¹æ¡ˆ 1: ä½¿ç”¨ S3 Control API (ListRegionalBuckets)** â­ æ¨è

```python
# replimap/scanners/s3_scanner.py (ä¿®æ”¹)
def _scan_buckets(self, s3: Any, graph: GraphEngine) -> None:
    """Scan all S3 buckets with support for 1000+ buckets."""
    logger.debug("Listing S3 buckets...")

    # ğŸ”¥ ä½¿ç”¨ S3 Control API æ›¿ä»£ list_buckets
    account_id = self._get_account_id_from_sts()
    s3control = self.session.client('s3control')

    buckets_to_process: list[str] = []

    try:
        # S3 Control API æ”¯æŒåˆ†é¡µ
        paginator = s3control.get_paginator('list_regional_buckets')
        for page in paginator.paginate(AccountId=account_id, OutpostId=''):
            for bucket in page.get('RegionalBucketList', []):
                bucket_name = bucket['Name']
                bucket_region = bucket['Region']

                # åªå¤„ç†ç›®æ ‡ region çš„æ¡¶
                if bucket_region != self.region:
                    continue

                buckets_to_process.append(bucket_name)

    except ClientError as e:
        # Fallback åˆ° list_buckets (æœ‰ 1000 é™åˆ¶)
        logger.warning(
            f"S3 Control API failed ({e}), falling back to list_buckets (max 1000 buckets)"
        )
        response = s3.list_buckets()
        for bucket in response.get("Buckets", []):
            bucket_name = bucket["Name"]

            # Get bucket region
            try:
                location = s3.get_bucket_location(Bucket=bucket_name)
                bucket_region = location.get("LocationConstraint") or "us-east-1"
            except ClientError as e:
                logger.warning(f"Could not get region for bucket {bucket_name}: {e}")
                continue

            if bucket_region != self.region:
                continue

            buckets_to_process.append(bucket_name)

    if len(buckets_to_process) >= 1000:
        logger.warning(
            f"âš ï¸ Found exactly 1000 S3 buckets - this may indicate the API limit was hit. "
            f"Some buckets may be missing. Consider using S3 Control API."
        )

    logger.debug(f"Processing {len(buckets_to_process)} S3 buckets...")

    # ... åç»­å¹¶è¡Œå¤„ç†é€»è¾‘ä¸å˜
```

**æ–¹æ¡ˆ 2: æ–‡æ¡£åŒ–é™åˆ¶ + è­¦å‘Š**

å¦‚æœä¸ä¿®æ”¹ä»£ç ï¼Œè‡³å°‘è¦æ–‡æ¡£åŒ–ï¼š

```python
def _scan_buckets(self, s3: Any, graph: GraphEngine) -> None:
    """
    Scan all S3 buckets with parallel processing.

    âš ï¸ WARNING: Due to AWS API limitations, this scanner can only detect
    the first 1000 S3 buckets in your account. If you have more than 1000
    buckets, some will be silently skipped.

    Workaround: Use AWS Organizations or S3 Control API for large accounts.
    """
    logger.debug("Listing S3 buckets...")

    response = s3.list_buckets()
    buckets = response.get("Buckets", [])

    if len(buckets) == 1000:
        logger.error(
            "ğŸ”´ CRITICAL: Detected exactly 1000 S3 buckets. "
            "AWS list_buckets API has a hard limit of 1000. "
            "If your account has more buckets, they will NOT be scanned. "
            "Please contact RepliMap support for large account support."
        )

    # ...
```

### å·¥ä½œé‡ä¼°ç®—

**æ–¹æ¡ˆ 1 (S3 Control API)**:
- **å¼€å‘æ—¶é—´**: 1 å¤©
  - å®ç° S3 Control API è°ƒç”¨: 3 å°æ—¶
  - Fallback é€»è¾‘: 2 å°æ—¶
  - æµ‹è¯• (éœ€è¦ >1000 æ¡¶è´¦æˆ·): 3 å°æ—¶
- **æ€»è®¡**: **1 å¤©** (2 story points)

**æ–¹æ¡ˆ 2 (æ–‡æ¡£åŒ–)**:
- **å¼€å‘æ—¶é—´**: 1 å°æ—¶
- **é£é™©**: ç”¨æˆ·å¯èƒ½ä»ç„¶ä¸çŸ¥é“é™åˆ¶

---

## [FINDING-PG003] ComputeScanner åµŒå¥— API è°ƒç”¨æ— ç‹¬ç«‹é”™è¯¯å¤„ç†

**ä¸¥é‡ç¨‹åº¦**: High
**ä¼˜å…ˆçº§**: P1
**ç±»åˆ«**: Reliability
**ç»„ä»¶**: [replimap/scanners/compute_scanner.py](../replimap/scanners/compute_scanner.py):80-136

### æè¿°

`ComputeScanner` åœ¨åˆ†é¡µå¾ªç¯å†…è°ƒç”¨é¢å¤–çš„ API (æ— åˆ†é¡µå™¨)ï¼Œè¿™äº›è°ƒç”¨å¤±è´¥ä¼šå¯¼è‡´éƒ¨åˆ†èµ„æºä¸¢å¤±ã€‚

```python
# replimap/scanners/compute_scanner.py:80-136
def _scan_launch_templates(self, graph: GraphEngine) -> None:
    """Scan all Launch Templates in the region."""
    ec2 = self.get_client("ec2")

    paginator = ec2.get_paginator("describe_launch_templates")
    for page in paginator.paginate():  # âŒ æ— é”™è¯¯å¤„ç†
        for lt in page.get("LaunchTemplates", []):
            lt_id = lt["LaunchTemplateId"]

            # ğŸ”´ é—®é¢˜ï¼šè¿™ä¸ªè°ƒç”¨å¤±è´¥ â†’ æ•´ä¸ª Launch Template æ‰«æä¸­æ­¢
            version_resp = ec2.describe_launch_template_versions(
                LaunchTemplateId=lt_id,
                Versions=["$Latest"],
            )  # âŒ æ—  try/except

            versions = version_resp.get("LaunchTemplateVersions", [])
            lt_data = versions[0].get("LaunchTemplateData", {}) if versions else {}

            # ...
```

**é—®é¢˜**:
- å¦‚æœ `describe_launch_template_versions` å¤±è´¥ (æƒé™é—®é¢˜ã€API throttling)
- å¼‚å¸¸å‘ä¸Šä¼ æ’­ â†’ æ•´ä¸ª Launch Template æ‰«æä¸­æ­¢
- å‰é¢å·²æ‰«æçš„ Launch Templates **å¯èƒ½** åœ¨å›¾ä¸­ï¼Œä½†æ‰«ææ ‡è®°ä¸ºå¤±è´¥

### å½±å“

- å•ä¸ª Launch Template çš„ç‰ˆæœ¬è·å–å¤±è´¥ â†’ æ‰€æœ‰ LT æ‰«æå¤±è´¥
- ç”¨æˆ·ä½“éªŒå·® (ä¸ºä»€ä¹ˆæ‰«æå¤±è´¥ï¼Ÿ)

### æ¨èä¿®å¤

```python
def _scan_launch_templates(self, graph: GraphEngine) -> None:
    """Scan all Launch Templates in the region."""
    ec2 = self.get_client("ec2")

    paginator = ec2.get_paginator("describe_launch_templates")

    # ğŸ”¥ ä½¿ç”¨ resilient_paginate
    result = resilient_paginate(
        paginator.paginate(),
        "LaunchTemplates",
        on_page_success=lambda lts: self._process_launch_templates(ec2, lts, graph)
    )

    if result.failed_pages > 0:
        logger.warning(
            f"Launch Template scan: {result.successful_pages}/{result.total_pages} pages succeeded"
        )

def _process_launch_templates(self, ec2, launch_templates: list, graph: GraphEngine):
    """å¤„ç† Launch Template é¡µé¢"""
    for lt in launch_templates:
        lt_id = lt["LaunchTemplateId"]
        lt_name = lt["LaunchTemplateName"]
        tags = self._extract_tags(lt.get("Tags"))

        # ğŸ”¥ ç‹¬ç«‹é”™è¯¯å¤„ç†
        lt_data = {}
        try:
            version_resp = ec2.describe_launch_template_versions(
                LaunchTemplateId=lt_id,
                Versions=["$Latest"],
            )
            versions = version_resp.get("LaunchTemplateVersions", [])
            lt_data = versions[0].get("LaunchTemplateData", {}) if versions else {}
        except ClientError as e:
            logger.warning(
                f"Could not get version for Launch Template {lt_name}: {e}. "
                f"Continuing with basic info..."
            )

        # å³ä½¿ç‰ˆæœ¬è·å–å¤±è´¥ï¼Œä»ç„¶æ·»åŠ åŸºæœ¬ä¿¡æ¯
        node = ResourceNode(
            id=lt_id,
            resource_type=ResourceType.LAUNCH_TEMPLATE,
            region=self.region,
            config={
                "name": lt_name,
                "default_version": lt.get("DefaultVersionNumber"),
                "latest_version": lt.get("LatestVersionNumber"),
                # å¦‚æœ lt_data ä¸ºç©ºï¼Œè¿™äº›å­—æ®µä¸º None
                "instance_type": lt_data.get("InstanceType"),
                "image_id": lt_data.get("ImageId"),
                # ...
            },
            arn=f"arn:aws:ec2:{self.region}::launch-template/{lt_id}",
            tags=tags,
        )

        graph.add_resource(node)
        logger.debug(f"Added Launch Template: {lt_name}")
```

### å·¥ä½œé‡ä¼°ç®—

- **å¼€å‘æ—¶é—´**: 4 å°æ—¶ (ä¿®æ”¹ ComputeScanner çš„ 4 ä¸ªæ–¹æ³•)
- **æµ‹è¯•æ—¶é—´**: 2 å°æ—¶
- **æ€»è®¡**: **0.75 å¤©** (1.5 story points)

---

## [FINDING-PG004] æ— åˆ†é¡µè¿›åº¦åé¦ˆï¼Œå¤§è´¦æˆ·æ‰«æçœ‹èµ·æ¥å¡æ­»

**ä¸¥é‡ç¨‹åº¦**: Medium
**ä¼˜å…ˆçº§**: P2
**ç±»åˆ«**: User Experience
**ç»„ä»¶**: æ‰€æœ‰æ‰«æå™¨

### æè¿°

æ‰«æå¤§å‹è´¦æˆ·æ—¶ï¼Œç”¨æˆ·çœ‹åˆ°ï¼š

```
INFO: Scanning EC2 instances in us-east-1...
[ç­‰å¾… 5 åˆ†é’Ÿï¼Œæ²¡æœ‰ä»»ä½•è¾“å‡º]
INFO: EC2 scanning complete
```

ç”¨æˆ·ä¸çŸ¥é“ï¼š
- æ‰«æè¿›åº¦å¦‚ä½• (20% è¿˜æ˜¯ 80%?)
- æ˜¯å¦å¡ä½äº†
- è¿˜éœ€è¦ç­‰å¤šä¹…

### å½±å“

- ç”¨æˆ·ä½“éªŒå·®
- ç”¨æˆ·å¯èƒ½è¯¯è®¤ä¸ºå·¥å…·å¡æ­»ï¼Œå¼ºåˆ¶ç»ˆæ­¢æ‰«æ

### æ¨èä¿®å¤

**æ–¹æ¡ˆ 1: é€é¡µè¿›åº¦æ—¥å¿—**

```python
# replimap/scanners/base.py
def resilient_paginate(...) -> PaginationResult:
    result = PaginationResult()
    page_iterator = iter(paginator)

    # ğŸ”¥ ä¼°ç®—æ€»é¡µæ•° (å¦‚æœ API æä¾›)
    estimated_total = None

    while True:
        page_num = result.total_pages + 1

        for attempt in range(max_retries + 1):
            try:
                page = next(page_iterator)
                result.total_pages += 1

                items = page.get(result_key, [])
                result.items_collected += len(items)

                # ğŸ”¥ è¿›åº¦åé¦ˆ
                if estimated_total:
                    progress_pct = (page_num / estimated_total) * 100
                    logger.info(
                        f"[{page_num}/{estimated_total}] {progress_pct:.0f}% "
                        f"({result.items_collected} items collected)"
                    )
                else:
                    logger.info(
                        f"[Page {page_num}] {result.items_collected} items collected"
                    )

                # ...
```

**æ–¹æ¡ˆ 2: Rich è¿›åº¦æ¡**

```python
from rich.progress import Progress

def _scan_instances(self, ec2, graph):
    paginator = ec2.get_paginator("describe_instances")

    with Progress() as progress:
        # ğŸ”¥ å¦‚æœä¸çŸ¥é“æ€»æ•°ï¼Œä½¿ç”¨ indeterminate è¿›åº¦æ¡
        task = progress.add_task("[cyan]Scanning EC2 instances...", total=None)

        page_num = 0
        for page in paginator.paginate():
            page_num += 1
            instances_in_page = sum(
                len(r['Instances']) for r in page.get('Reservations', [])
            )

            # æ›´æ–°è¿›åº¦
            progress.update(
                task,
                description=f"[cyan]Page {page_num} ({instances_in_page} instances)..."
            )

            # å¤„ç†å®ä¾‹
            for reservation in page.get("Reservations", []):
                for instance in reservation.get("Instances", []):
                    self._process_instance(instance, graph)

        progress.update(task, completed=True)
```

### å·¥ä½œé‡ä¼°ç®—

- **å¼€å‘æ—¶é—´**: 1 å¤© (æ‰€æœ‰æ‰«æå™¨)
- **æ€»è®¡**: **1 å¤©** (2 story points)

---

## [FINDING-PG005] åˆ†é¡µå¤±è´¥æœªè®°å½•è¯¦ç»†ä¸Šä¸‹æ–‡

**ä¸¥é‡ç¨‹åº¦**: Low
**ä¼˜å…ˆçº§**: P2
**ç±»åˆ«**: Observability
**ç»„ä»¶**: æ‰€æœ‰æ‰«æå™¨

### æè¿°

åˆ†é¡µå¤±è´¥æ—¶ï¼Œæ—¥å¿—åªæ˜¾ç¤ºï¼š

```
ERROR: VPC scanning failed: Read timeout on endpoint URL
```

ç¼ºå°‘å…³é”®ä¸Šä¸‹æ–‡ï¼š
- å“ªä¸€é¡µå¤±è´¥ï¼Ÿ
- å·²æ‰«æå¤šå°‘èµ„æºï¼Ÿ
- å¤±è´¥å‰çš„åˆ†é¡µ token æ˜¯ä»€ä¹ˆï¼Ÿ

### æ¨èä¿®å¤

```python
def resilient_paginate(...) -> PaginationResult:
    # ...

    except Exception as e:
        last_error = e

        if attempt < max_retries:
            logger.warning(
                f"Page {page_num} failed (attempt {attempt + 1}/{max_retries + 1}), "
                f"retrying in {backoff:.1f}s: {e}"
            )
        else:
            # ğŸ”¥ è¯¦ç»†é”™è¯¯æ—¥å¿—
            logger.error(
                f"âŒ Page {page_num} failed after {max_retries + 1} attempts\n"
                f"  Error: {e}\n"
                f"  Items collected so far: {result.items_collected}\n"
                f"  Successful pages: {result.successful_pages}\n"
                f"  Failed pages: {result.failed_pages + 1}\n"
                f"  Last NextToken: {page.get('NextToken', 'N/A')}"
            )
            result.failed_pages += 1
            result.errors.append(last_error)
```

### å·¥ä½œé‡ä¼°ç®—

- **å¼€å‘æ—¶é—´**: 2 å°æ—¶
- **æ€»è®¡**: **0.25 å¤©** (0.5 story points)

---

## [FINDING-PG006] æ— åˆ†é¡µæ€§èƒ½ä¼˜åŒ– (é¢„å–ã€æ‰¹å¤„ç†)

**ä¸¥é‡ç¨‹åº¦**: Low
**ä¼˜å…ˆçº§**: P3
**ç±»åˆ«**: Performance
**ç»„ä»¶**: æ‰€æœ‰æ‰«æå™¨

### æè¿°

å½“å‰åˆ†é¡µé€»è¾‘æ˜¯ä¸²è¡Œçš„ï¼š

```
è¯·æ±‚é¡µ1 â†’ ç­‰å¾…å“åº” â†’ å¤„ç† â†’ è¯·æ±‚é¡µ2 â†’ ç­‰å¾…å“åº” â†’ å¤„ç† â†’ ...
```

å¯ä»¥ä¼˜åŒ–ä¸ºï¼š

```
è¯·æ±‚é¡µ1 + é¡µ2 â†’ ç­‰å¾…é¡µ1å“åº” â†’ å¤„ç†é¡µ1 + è¯·æ±‚é¡µ3 â†’ ç­‰å¾…é¡µ2å“åº” â†’ å¤„ç†é¡µ2 + è¯·æ±‚é¡µ4 â†’ ...
```

### æ¨èä¿®å¤

ä½¿ç”¨ `asyncio` é¢„å–ä¸‹ä¸€é¡µï¼š

```python
async def async_resilient_paginate(
    async_paginator,
    result_key: str,
    on_page_success,
    prefetch_pages: int = 2,
):
    """å¼‚æ­¥åˆ†é¡µå™¨ï¼Œæ”¯æŒé¢„å–"""
    result = PaginationResult()
    page_queue = asyncio.Queue(maxsize=prefetch_pages)

    # Producer: é¢„å–é¡µé¢
    async def fetch_pages():
        async for page in async_paginator:
            await page_queue.put(page)
        await page_queue.put(None)  # Sentinel

    # Consumer: å¤„ç†é¡µé¢
    async def process_pages():
        while True:
            page = await page_queue.get()
            if page is None:
                break

            items = page.get(result_key, [])
            result.items_collected += len(items)

            if on_page_success:
                on_page_success(items)

            result.successful_pages += 1

    # å¹¶å‘æ‰§è¡Œ
    await asyncio.gather(
        fetch_pages(),
        process_pages()
    )

    return result
```

### å·¥ä½œé‡ä¼°ç®—

- **å¼€å‘æ—¶é—´**: 3 å¤© (éœ€è¦è¿ç§»åˆ°å¼‚æ­¥)
- **æ”¶ç›Š**: æ‰«æé€Ÿåº¦æå‡ 20-30%
- **ä¼˜å…ˆçº§**: P3 (æ€§èƒ½ä¼˜åŒ–)

---

## æ€»ä½“ä¿®å¤è·¯çº¿å›¾ (Fix Roadmap)

### çŸ­æœŸ (1 å‘¨å†… - P0)

1. âœ… **[PG001] resilient_paginate + éƒ¨åˆ†æˆåŠŸæŠ¥å‘Š** (3-4 å¤©)
   - å®ç°æ ¸å¿ƒ `resilient_paginate()` å‡½æ•°
   - ä¿®æ”¹ VPCScanner, EC2Scanner, RDSScanner (é«˜é£é™©)
   - å•å…ƒæµ‹è¯• + é›†æˆæµ‹è¯•

2. âœ… **[PG002] S3 list_buckets ä¿®å¤** (1 å¤©)
   - ä½¿ç”¨ S3 Control API
   - Fallback é€»è¾‘

### ä¸­æœŸ (2-4 å‘¨ - P1)

3. âœ… **[PG003] åµŒå¥— API è°ƒç”¨é”™è¯¯å¤„ç†** (0.75 å¤©)
   - ä¿®å¤ ComputeScanner

4. âœ… **[PG004] åˆ†é¡µè¿›åº¦åé¦ˆ** (1 å¤©)
   - æ·»åŠ  Rich è¿›åº¦æ¡æˆ–æ—¥å¿—

5. â¸ **[PG005] è¯¦ç»†é”™è¯¯æ—¥å¿—** (0.25 å¤©)

### é•¿æœŸ (3 ä¸ªæœˆ+ - P2/P3)

6. ğŸ”„ **åˆ†é¡µçŠ¶æ€æŒä¹…åŒ– (æ£€æŸ¥ç‚¹æ¢å¤)**
7. ğŸ”„ **å¼‚æ­¥é¢„å–ä¼˜åŒ–**

---

## éªŒè¯è®¡åˆ’ (Verification Plan)

### å•å…ƒæµ‹è¯•

```python
# tests/test_pagination.py
import pytest
from unittest.mock import MagicMock, Mock
from botocore.exceptions import ClientError
from replimap.scanners.base import resilient_paginate, PaginationResult

def test_resilient_paginate_all_success():
    """éªŒè¯æ‰€æœ‰é¡µé¢æˆåŠŸçš„åœºæ™¯"""
    # Mock paginator
    pages = [
        {'Vpcs': [{'VpcId': f'vpc-{i}'} for i in range(10)]},
        {'Vpcs': [{'VpcId': f'vpc-{i}'} for i in range(10, 20)]},
        {'Vpcs': [{'VpcId': f'vpc-{i}'} for i in range(20, 25)]},
    ]

    paginator = iter(pages)

    collected_vpcs = []
    result = resilient_paginate(
        paginator,
        'Vpcs',
        on_page_success=lambda vpcs: collected_vpcs.extend(vpcs)
    )

    assert result.total_pages == 3
    assert result.successful_pages == 3
    assert result.failed_pages == 0
    assert result.items_collected == 25
    assert result.success_rate == 1.0
    assert len(collected_vpcs) == 25

def test_resilient_paginate_partial_failure():
    """éªŒè¯éƒ¨åˆ†é¡µé¢å¤±è´¥çš„åœºæ™¯"""

    class FailingPaginator:
        def __init__(self):
            self.call_count = 0

        def __iter__(self):
            return self

        def __next__(self):
            self.call_count += 1
            if self.call_count == 2:
                # ç¬¬2é¡µå¤±è´¥
                raise ClientError(
                    {'Error': {'Code': 'RequestTimeout', 'Message': 'Timeout'}},
                    'DescribeVpcs'
                )
            if self.call_count > 3:
                raise StopIteration

            return {'Vpcs': [{'VpcId': f'vpc-{self.call_count}'}]}

    paginator = FailingPaginator()

    collected_vpcs = []
    result = resilient_paginate(
        paginator,
        'Vpcs',
        on_page_success=lambda vpcs: collected_vpcs.extend(vpcs),
        max_retries=1  # å¿«é€Ÿå¤±è´¥
    )

    assert result.total_pages == 3  # å°è¯•äº†3é¡µ
    assert result.successful_pages == 2  # é¡µ1å’Œé¡µ3æˆåŠŸ
    assert result.failed_pages == 1  # é¡µ2å¤±è´¥
    assert result.items_collected == 2
    assert 0.6 < result.success_rate < 0.7  # 2/3

def test_s3_scanner_1000_bucket_warning():
    """éªŒè¯ S3 scanner æ£€æµ‹åˆ°1000æ¡¶æ—¶å‘å‡ºè­¦å‘Š"""
    # TODO: å®ç°
```

### é›†æˆæµ‹è¯•

```bash
# å¤§è§„æ¨¡è´¦æˆ·æµ‹è¯•
export AWS_PROFILE=large-test-account  # 500+ EC2, 1000+ S3 buckets

# æ¨¡æ‹Ÿç½‘ç»œä¸ç¨³å®š
sudo tc qdisc add dev eth0 root netem loss 5% delay 100ms 10ms

replimap -p large-test-account -r us-east-1 scan --verbose

# éªŒè¯æŒ‡æ ‡:
# âœ… éƒ¨åˆ†æˆåŠŸæ¶ˆæ¯: "EC2 scan: 23/25 pages succeeded (460 instances, 2 pages failed)"
# âœ… S3 è­¦å‘Š: "Found exactly 1000 S3 buckets - API limit may be hit"
# âœ… æ‰«ææœªå®Œå…¨ä¸­æ­¢
# âœ… ç”Ÿæˆçš„ Terraform åŒ…å«å¤§éƒ¨åˆ†èµ„æº

# æ¸…ç†ç½‘ç»œè®¾ç½®
sudo tc qdisc del dev eth0 root
```

### Chaos Engineering æµ‹è¯•

```python
# tests/chaos/test_pagination_chaos.py
import random
from botocore.exceptions import ClientError

class ChaosMonkeyPaginator:
    """æ¨¡æ‹Ÿéšæœºåˆ†é¡µå¤±è´¥"""

    def __init__(self, pages: list, failure_rate: float = 0.2):
        self.pages = pages
        self.failure_rate = failure_rate
        self.index = 0

    def __iter__(self):
        return self

    def __next__(self):
        if self.index >= len(self.pages):
            raise StopIteration

        # éšæœºå¤±è´¥
        if random.random() < self.failure_rate:
            self.index += 1
            raise ClientError(
                {'Error': {'Code': 'InternalError', 'Message': 'Chaos!'}},
                'DescribeVpcs'
            )

        page = self.pages[self.index]
        self.index += 1
        return page

def test_chaos_pagination():
    """æ··æ²Œæµ‹è¯•ï¼šéšæœº20%é¡µé¢å¤±è´¥"""
    pages = [{'Vpcs': [f'vpc-{i}']} for i in range(50)]  # 50 é¡µ

    collected = []
    result = resilient_paginate(
        ChaosMonkeyPaginator(pages, failure_rate=0.2),
        'Vpcs',
        on_page_success=lambda vpcs: collected.extend(vpcs),
        max_retries=2
    )

    # è‡³å°‘60%é¡µé¢åº”è¯¥æˆåŠŸ
    assert result.success_rate >= 0.6
    # æ”¶é›†åˆ°çš„æ•°æ®åº”è¯¥å¤§äº0
    assert len(collected) > 0
```

### æ€§èƒ½åŸºå‡†æµ‹è¯•

| åœºæ™¯ | ä¿®å¤å‰ | ä¿®å¤å (ç›®æ ‡) |
|------|--------|--------------|
| **EC2 æ‰«æ (500 å®ä¾‹)** | å…¨å¤±è´¥æˆ–å…¨æˆåŠŸ | éƒ¨åˆ†æˆåŠŸ (>80% é¡µé¢) |
| **åˆ†é¡µå¤±è´¥æ¢å¤æ—¶é—´** | N/A (æ•´ä½“é‡æ–°æ‰«æ) | åªé‡è¯•å¤±è´¥é¡µ |
| **S3 æ¡¶æ‰«æ (1500 æ¡¶)** | åªæ‰«æ 1000 | æ‰«æå…¨éƒ¨ 1500 |
| **ç”¨æˆ·å¯è§è¿›åº¦** | æ—  | Rich è¿›åº¦æ¡ |

---

## å‚è€ƒèµ„æ–™ (References)

1. **AWS åˆ†é¡µæœ€ä½³å®è·µ**:
   - [Paginating AWS API Results](https://docs.aws.amazon.com/cli/latest/userguide/cli-usage-pagination.html)

2. **S3 list_buckets é™åˆ¶**:
   - [ListBuckets API Reference](https://docs.aws.amazon.com/AmazonS3/latest/API/API_ListBuckets.html)
   - [S3 Control API - ListRegionalBuckets](https://docs.aws.amazon.com/AmazonS3/latest/API/API_control_ListRegionalBuckets.html)

3. **Resilience æ¨¡å¼**:
   - [Circuit Breaker Pattern](https://martinfowler.com/bliki/CircuitBreaker.html)
   - [Retry Pattern with Exponential Backoff](https://docs.microsoft.com/en-us/azure/architecture/patterns/retry)

---

**æŠ¥å‘Šç”Ÿæˆæ—¶é—´**: 2026-01-11 00:30 UTC+13
**å®¡æŸ¥å‘˜**: Claude Sonnet 4.5 (via Claude Code)
**æ‰¹å‡†çŠ¶æ€**: å¾…ç”¨æˆ·ç¡®è®¤
